{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78580,"status":"ok","timestamp":1684038094801,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"DEk5q0rhjBWZ","outputId":"f4af5470-bc05-4e51-ffc7-d7f0efff65f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying gs://indaba-data/README.txt...\n","/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test_mut.pt...\n","/ [0 files][    0.0 B/  9.3 MiB]                                                \rCopying gs://indaba-data/test/test.csv...\n","/ [0 files][    0.0 B/  9.6 MiB]                                                \rCopying gs://indaba-data/test/test_wt.pt...\n","/ [0 files][    0.0 B/ 18.9 MiB]                                                \rCopying gs://indaba-data/train/train.csv...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_wt.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_mut.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==\u003e NOTE: You are downloading one or more large file(s), which would\n","run significantly faster if you enabled sliced object downloads. This\n","feature is enabled by default but requires that compiled crcmod be\n","installed (see \"gsutil help crcmod\").\n","\n"]}],"source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5056,"status":"ok","timestamp":1684038099852,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"Jvd8ERpgTvji","outputId":"c9f32275-65b9-4c58-b88f-36055026c8d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/indaba-data\n"]}],"source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","\n","# move to data folder\n","%cd indaba-data"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3719,"status":"ok","timestamp":1684038103566,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"36ZgVoj5odV4"},"outputs":[],"source":["# Load Embedding tensors \u0026 Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1684038103567,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"7O4dRi72DA_K","outputId":"ba81a4c0-b885-40e9-ccda-45be0f1d009f"},"outputs":[{"data":{"text/plain":["339778"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["len(df)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3584,"status":"ok","timestamp":1684038107141,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"0QhdR9Tq3Acm"},"outputs":[],"source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.21, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684038107141,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"NOtYzkrWHPz8","outputId":"c64d35a5-3c40-4c16-dfd7-5a62fcac9e1f"},"outputs":[{"data":{"text/plain":["8389"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataloader)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684038107142,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"lNRejSIXHTOs","outputId":"af4a7abf-0201-4f39-9b51-49c1134845b8"},"outputs":[{"data":{"text/plain":["2230"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(val_dataloader)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1684038107653,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"qBorXcDfU8MN","outputId":"21a8e6f6-2516-450a-fcd3-532a5824f0f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n","wt_emb shape: torch.Size([32, 1280]), mut_emb shape: torch.Size([32, 1280])\n"]}],"source":["for i, (wt_emb, mut_emb, ddg) in enumerate(train_dataloader):\n","    print(f'wt_emb shape: {wt_emb.shape}, mut_emb shape: {mut_emb.shape}')\n","    if i \u003e 5:  # Print shapes for first 5 batches only\n","        break\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1684038124692,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"sT81Nq68VqiF","outputId":"69a067a9-fd74-4bd2-ef09-2622283792f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([268424, 1280]) torch.Size([268424, 1280]) torch.Size([32, 1])\n"]}],"source":["for wt_emb, mut_emb, ddg in train_dataloader:\n","    print(wt_emb_train.shape, mut_emb_train.shape, ddg.shape)\n","    break\n","import torch.nn as nn\n","import torch.nn.functional as F\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1134,"status":"ok","timestamp":1684038127894,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"NFlVrUgZZ_Um"},"outputs":[],"source":["class AttentionLayer(nn.Module):\n","    def __init__(self, input_dim, attention_dim):\n","        super(AttentionLayer, self).__init__()\n","        self.attention_weights = nn.Parameter(torch.randn(input_dim, attention_dim))\n","        self.fc = nn.Linear(attention_dim, 1)\n","\n","    def forward(self, x):\n","        attention_scores = F.softmax(self.fc(x @ self.attention_weights), dim=1)\n","        return (attention_scores * x)\n","\n","class MLPWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, attention_dim):\n","        super(MLPWithAttention, self).__init__()\n","        self.attention = AttentionLayer(input_dim, attention_dim)\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        x = torch.cat((wt_emb, mut_emb), dim=1)\n","        x = self.attention(x)\n","        x = x.view(x.size(0), -1)  # Flatten the tensor\n","        x = F.relu(self.fc1(x))\n","        out = self.fc2(x)\n","        return out\n","\n","# Create the model\n","input_dim = 1280 * 2  # Multiply by 2 since we're concatenating wt_emb and mut_emb\n","hidden_dim = 256\n","output_dim = 1  # ddG value\n","attention_dim = 512  # Hyperparameter that you may need to tune\n","model = MLPWithAttention(input_dim, hidden_dim, output_dim, attention_dim)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"pNdh0mHLT76j"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.9231522999555565, Val Loss: 0.8415401782489679\n","Epoch 2, Train Loss: 0.8382683936539815, Val Loss: 0.8188282968551589\n","Epoch 3, Train Loss: 0.8246590296788584, Val Loss: 0.8031103128943208\n","Epoch 4, Train Loss: 0.8174986777977461, Val Loss: 0.8277799490855948\n","Epoch 5, Train Loss: 0.8142003603754744, Val Loss: 0.8161756578314999\n","Epoch 6, Train Loss: 0.8104593254610699, Val Loss: 0.8039739552768357\n","Epoch 7, Train Loss: 0.8071092830335643, Val Loss: 0.8047313353138654\n","Epoch 8, Train Loss: 0.8047898599645039, Val Loss: 0.8521381614053196\n"]}],"source":["# Training loop\n","import torch.nn.functional as F\n","num_epochs = 15  # Adjust as needed\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_losses = []\n","\n","    for wt_emb, mut_emb, ddg in train_dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(wt_emb, mut_emb)\n","        loss = criterion(outputs, ddg)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","    \n","    avg_train_loss = sum(train_losses) / len(train_losses)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_losses = []\n","        for wt_emb, mut_emb, ddg in val_dataloader:\n","            outputs = model(wt_emb, mut_emb)\n","            loss = criterion(outputs, ddg)\n","            val_losses.append(loss.item())\n","    \n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1684038108347,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"1Qy8OdmpKQOK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1684038108347,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"kpRChemTK2NO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1684038108347,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"r2xqF9KKK2Dm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9GDKutS_nKOT"},"source":["## Prediction \u0026 submission"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1684038108350,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"ave_DDMp8fo9"},"outputs":[],"source":["# load embedding tensors \u0026 traing csv\n","wt_test_emb = torch.load(\"test/test_wt.pt\")\n","mut_test_emb = torch.load(\"test/test_mut.pt\")\n","df_test = pd.read_csv(\"test/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1684038108350,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"9Xmav2yhm_Di"},"outputs":[],"source":["# creating testing dataset and loading the embedding\n","test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n","# preparing a dataloader for the testing\n","test_dataloader = torch.utils.data.dataloader.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        shuffle=False,\n","        num_workers=2,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1684038108350,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"DiylsXvjqOul"},"outputs":[],"source":["df_result = pd.DataFrame()\n","with torch.no_grad():\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n","    x1 = data_wt.to(device)\n","    x2 = data_mut.to(device)\n","    id = target.to(device)\n","    # make prediction\n","    y_pred = model(x1,x2)\n","    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1684038108351,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"FPm-a2USexgw"},"outputs":[],"source":["df_result.to_csv(\"submission.csv\",index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1684038108351,"user":{"displayName":"Bida Bamk","userId":"06775854723650008435"},"user_tz":-60},"id":"7PGmPkRmezal"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}