{"cells":[{"cell_type":"code","source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"],"metadata":{"id":"DEk5q0rhjBWZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cfe59d3-4878-4f29-e163-bf63472e8fc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://indaba-data/README.txt...\n","/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test.csv...\n","Copying gs://indaba-data/test/test_wt.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train.csv...\n","Copying gs://indaba-data/test/test_mut.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_mut.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_wt.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n","run significantly faster if you enabled sliced object downloads. This\n","feature is enabled by default but requires that compiled crcmod be\n","installed (see \"gsutil help crcmod\").\n","\n"]}]},{"cell_type":"code","source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","\n","# move to data folder\n","%cd indaba-data"],"metadata":{"id":"Jvd8ERpgTvji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f13e524-ac95-4d2d-a487-72793834dff4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/indaba-data\n"]}]},{"cell_type":"code","source":["# Load Embedding tensors & Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"],"metadata":{"id":"36ZgVoj5odV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=27, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=27, shuffle=False, num_workers=2)\n"],"metadata":{"id":"0QhdR9Tq3Acm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOtYzkrWHPz8","outputId":"4dcd5d9a-dfda-4439-e170-9f69f3d91708"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10455"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["len(val_dataloader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNRejSIXHTOs","outputId":"d4c997c0-5ed4-4669-ac15-33dae405f0e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2614"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for i, (wt_emb, mut_emb, ddg) in enumerate(train_dataloader):\n","    print(f'wt_emb shape: {wt_emb.shape}, mut_emb shape: {mut_emb.shape}')\n","    if i > 5:  # Print shapes for first 5 batches only\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBorXcDfU8MN","outputId":"1d67e346-0fa2-4b58-aeed-ef009c200c3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n"]}]},{"cell_type":"code","source":["for wt_emb, mut_emb, ddg in train_dataloader:\n","    print(wt_emb_train.shape, mut_emb_train.shape, ddg.shape)\n","    break\n","import torch.nn as nn\n","import torch.nn.functional as F\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT81Nq68VqiF","outputId":"f7a87f8b-732b-45c5-f7d5-18794c800ef1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([271822, 1280]) torch.Size([271822, 1280]) torch.Size([26, 1])\n"]}]},{"cell_type":"code","source":["class StabilityModel(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers):\n","        super(StabilityModel, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        \n","        # LSTM layer\n","        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        \n","        # Fully connected layers\n","        self.fc1 = torch.nn.Linear(hidden_dim * 2, 512)\n","        self.fc2 = torch.nn.Linear(512, 1)\n","\n","        # Dropout layer\n","        self.dp1 = torch.nn.Dropout(0.5)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        x = (wt_emb - mut_emb).unsqueeze(0)  # take the difference and add an extra dimension for the sequence\n","\n","        # Set initial hidden and cell states \n","        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device) # times 2 for bidirectional\n","        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device)\n","        \n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_dim*2)\n","        \n","        # Decode the hidden state of the last time step\n","        out = self.fc1(out[:, -1, :])\n","        out = F.relu(out)\n","        out = self.dp1(out)\n","        out = self.fc2(out)\n","\n","        return out\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = StabilityModel(input_dim=1280, hidden_dim=512, num_layers=2).to(device)\n","\n","# Loss and optimizer\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    for i, (wt_emb, mut_emb, ddg) in enumerate(train_dataloader):\n","        wt_emb = wt_emb.to(device)\n","        mut_emb = mut_emb.to(device)\n","        ddg = ddg.to(device)\n","\n","        # Forward pass\n","        outputs = model(wt_emb, mut_emb)\n","        loss = criterion(outputs, ddg)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n"],"metadata":{"id":"62hRgCNyCuGr","colab":{"base_uri":"https://localhost:8080/","height":435},"outputId":"6d27059f-501b-47ac-a93c-64881bd58df5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([26, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7ed11de8e2c0>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"kpRChemTK2NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r2xqF9KKK2Dm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction & submission"],"metadata":{"id":"9GDKutS_nKOT"}},{"cell_type":"code","source":["# load embedding tensors & traing csv\n","wt_test_emb = torch.load(\"test/test_wt.pt\")\n","mut_test_emb = torch.load(\"test/test_mut.pt\")\n","df_test = pd.read_csv(\"test/test.csv\")"],"metadata":{"id":"ave_DDMp8fo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating testing dataset and loading the embedding\n","test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n","# preparing a dataloader for the testing\n","test_dataloader = torch.utils.data.dataloader.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        shuffle=False,\n","        num_workers=2,\n","    )"],"metadata":{"id":"9Xmav2yhm_Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=model.to(device)\n","df_result = pd.DataFrame()\n","with torch.no_grad():\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n","    x1 = data_wt.to(device)\n","    x2 = data_mut.to(device)\n","    id = target.to(device)\n","    # make prediction\n","    y_pred = model(x1,x2)\n","    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"],"metadata":{"id":"DiylsXvjqOul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result.to_csv(\"submission_deepffnn.csv\",index=False)"],"metadata":{"id":"FPm-a2USexgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7PGmPkRmezal"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}