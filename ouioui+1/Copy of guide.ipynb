{"cells":[{"cell_type":"code","source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"],"metadata":{"id":"DEk5q0rhjBWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684028257363,"user_tz":-60,"elapsed":93415,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}},"outputId":"9f16b0e2-a2cf-4e5f-a196-03f48ec7ea64"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://indaba-data/README.txt...\n","/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test.csv...\n","/ [0 files][    0.0 B/290.0 KiB]                                                \rCopying gs://indaba-data/test/test_mut.pt...\n","/ [0 files][    0.0 B/  9.6 MiB]                                                \rCopying gs://indaba-data/test/test_wt.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train_mut.pt...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \rCopying gs://indaba-data/train/train.csv...\n","/ [0/9 files][    0.0 B/  3.3 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n","run significantly faster if you enabled sliced object downloads. This\n","feature is enabled by default but requires that compiled crcmod be\n","installed (see \"gsutil help crcmod\").\n","\n","Copying gs://indaba-data/train/train_wt.pt...\n"]}]},{"cell_type":"code","source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","\n","# move to data folder\n","%cd indaba-data"],"metadata":{"id":"Jvd8ERpgTvji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1d18bed-7268-4c9c-c27d-63c1c3347da0","executionInfo":{"status":"ok","timestamp":1684028261464,"user_tz":-60,"elapsed":4104,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/indaba-data\n"]}]},{"cell_type":"code","source":["# Load Embedding tensors & Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"],"metadata":{"id":"36ZgVoj5odV4","executionInfo":{"status":"ok","timestamp":1684028265172,"user_tz":-60,"elapsed":3717,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"],"metadata":{"id":"r4yfvBYXqJgS","executionInfo":{"status":"ok","timestamp":1684028268227,"user_tz":-60,"elapsed":3062,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class LSTMWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMWithAttention, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        \n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","\n","        # Attention mechanism\n","        self.attention = nn.Linear(hidden_dim, 1)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        # Pass the embeddings through the LSTM layer\n","        _, (hidden_state, _) = self.lstm(wt_emb)\n","\n","        if isinstance(hidden_state, tuple):\n","            # If hidden_state is a tuple (hidden_state, cell_state)\n","            hidden_state = hidden_state[0]\n","\n","        # Apply attention mechanism\n","        attn_weights = torch.softmax(self.attention(hidden_state), dim=1)\n","        context_vector = torch.bmm(attn_weights.unsqueeze(2), hidden_state.unsqueeze(1)).squeeze(1)\n","\n","        # Pass the context vector through the fully connected layer\n","        output = self.fc(context_vector)\n","        \n","        return output\n","\n"],"metadata":{"id":"HrbfsC8eqJXV","executionInfo":{"status":"ok","timestamp":1684028268228,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate the LSTM model with attention\n","input_dim = 1280  # Assuming the input dimensions of your embeddings are 1280\n","hidden_dim = 256\n","num_layers = 1\n","output_dim = 1\n","model = LSTMWithAttention(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","# Training parameters\n","epochs = 20\n","learning_rate = 0.001\n","# Define the loss function and optimizer\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    for i, (wt, mut, ddg) in enumerate(train_dataloader):\n","        wt, mut, ddg = wt.to(device), mut.to(device), ddg.to(device)\n","\n","        # Forward pass\n","        outputs = model(wt, mut)\n","        loss = criterion(outputs, ddg)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for i, (wt_val, mut_val, ddg_val) in enumerate(val_dataloader):\n","            wt_val, mut_val, ddg_val = wt_val.to(device), mut_val.to(device), ddg_val.to(device)\n","\n","            # Forward pass and calculate loss\n","            outputs = model(wt_val, mut_val)\n","            val_loss = criterion(outputs, ddg_val)\n","            val_losses.append(val_loss.item())\n","\n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item()}, Validation Loss: {avg_val_loss}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'lstm_with_attention.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":623},"id":"SR4uP1ikqJMz","outputId":"a1690871-b72c-4f56-f133-e7ed4fa80416","executionInfo":{"status":"error","timestamp":1684030870505,"user_tz":-60,"elapsed":1384257,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 0.8296712040901184, Validation Loss: 1.0834144611620184\n","Epoch 2/20, Train Loss: 1.0972990989685059, Validation Loss: 1.0835696757260689\n","Epoch 3/20, Train Loss: 1.887439250946045, Validation Loss: 1.0847988806089663\n","Epoch 4/20, Train Loss: 0.7520433068275452, Validation Loss: 1.1077480264246575\n","Epoch 5/20, Train Loss: 1.731468915939331, Validation Loss: 1.0842649564988869\n","Epoch 6/20, Train Loss: 1.1411679983139038, Validation Loss: 1.1029886196870813\n","Epoch 7/20, Train Loss: 1.2679224014282227, Validation Loss: 1.0835702061063825\n","Epoch 8/20, Train Loss: 1.622834324836731, Validation Loss: 1.085281296905039\n","Epoch 9/20, Train Loss: 1.124100923538208, Validation Loss: 1.0834729224864372\n","Epoch 10/20, Train Loss: 0.6484751105308533, Validation Loss: 1.0878175917711186\n","Epoch 11/20, Train Loss: 1.068366289138794, Validation Loss: 1.0837710908914016\n","Epoch 12/20, Train Loss: 1.2188291549682617, Validation Loss: 1.10397554833\n","Epoch 13/20, Train Loss: 0.939903736114502, Validation Loss: 1.084164467389413\n","Epoch 14/20, Train Loss: 1.3921786546707153, Validation Loss: 1.0931177269351684\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d4fc9991ef9e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Forward pass and calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmut_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddg_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-78abdbfdb079>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, wt_emb, mut_emb)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Apply attention mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"N-NekoVj5T_D","executionInfo":{"status":"ok","timestamp":1684029216261,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RBKyOZsG5TdK","executionInfo":{"status":"ok","timestamp":1684029216261,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"edYi37670Y07","executionInfo":{"status":"ok","timestamp":1684029216261,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nNEzUeNUzlKE","executionInfo":{"status":"ok","timestamp":1684029216262,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ahmed Hmila","userId":"02627900764628602826"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Prediction & submission"],"metadata":{"id":"9GDKutS_nKOT"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}