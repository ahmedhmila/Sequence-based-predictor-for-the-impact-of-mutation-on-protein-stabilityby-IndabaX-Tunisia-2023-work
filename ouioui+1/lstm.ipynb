{"cells":[{"cell_type":"code","source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"],"metadata":{"id":"DEk5q0rhjBWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684057041822,"user_tz":-120,"elapsed":18668,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}},"outputId":"8ea489ee-3fda-4f89-aca9-c16c3de4739e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://indaba-data/README.txt...\n","/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test.csv...\n","Copying gs://indaba-data/test/test_mut.pt...\n","Copying gs://indaba-data/test/test_wt.pt...\n","Copying gs://indaba-data/train/train.csv...\n","Copying gs://indaba-data/train/train_mut.pt...\n","Copying gs://indaba-data/train/train_wt.pt...\n","==> NOTE: You are downloading one or more large file(s), which would\n","run significantly faster if you enabled sliced object downloads. This\n","feature is enabled by default but requires that compiled crcmod be\n","installed (see \"gsutil help crcmod\").\n","\n","Exception in UIThread: \n","Caught CTRL-C (signal 2) - exiting\n","^C\n"]}]},{"cell_type":"code","source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","\n","# move to data folder\n","%cd indaba-data"],"metadata":{"id":"Jvd8ERpgTvji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7c393d8-02d7-4426-f347-a3a9aef41d43","executionInfo":{"status":"ok","timestamp":1684054727413,"user_tz":-60,"elapsed":4183,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/indaba-data\n"]}]},{"cell_type":"code","source":["# Load Embedding tensors & Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"],"metadata":{"id":"36ZgVoj5odV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O4dRi72DA_K","executionInfo":{"status":"ok","timestamp":1684054730773,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}},"outputId":"71db44a8-7549-4adc-f269-c9307392aa57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["339778"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=26, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=26, shuffle=False, num_workers=2)\n"],"metadata":{"id":"0QhdR9Tq3Acm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOtYzkrWHPz8","executionInfo":{"status":"ok","timestamp":1684054734453,"user_tz":-60,"elapsed":15,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}},"outputId":"4dcd5d9a-dfda-4439-e170-9f69f3d91708"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10455"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["len(val_dataloader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNRejSIXHTOs","executionInfo":{"status":"ok","timestamp":1684054734453,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}},"outputId":"d4c997c0-5ed4-4669-ac15-33dae405f0e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2614"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for i, (wt_emb, mut_emb, ddg) in enumerate(train_dataloader):\n","    print(f'wt_emb shape: {wt_emb.shape}, mut_emb shape: {mut_emb.shape}')\n","    if i > 5:  # Print shapes for first 5 batches only\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBorXcDfU8MN","executionInfo":{"status":"ok","timestamp":1684054735495,"user_tz":-60,"elapsed":1052,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}},"outputId":"08760a54-1a64-4b55-e003-43bc7d58eb06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n","wt_emb shape: torch.Size([26, 1280]), mut_emb shape: torch.Size([26, 1280])\n"]}]},{"cell_type":"code","source":["for wt_emb, mut_emb, ddg in train_dataloader:\n","    print(wt_emb_train.shape, mut_emb_train.shape, ddg.shape)\n","    break\n","import torch.nn as nn\n","import torch.nn.functional as F\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT81Nq68VqiF","executionInfo":{"status":"ok","timestamp":1684054735495,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ahmed Hmila","userId":"06370206958453082150"}},"outputId":"d29490b1-779c-45d3-9628-7a77d5837d73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([271822, 1280]) torch.Size([271822, 1280]) torch.Size([26, 1])\n"]}]},{"cell_type":"code","source":["class AdvancedLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate):\n","        super(AdvancedLSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        \n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True)\n","        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # times 2 for bidirectional\n","        self.dp1 = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        x = (wt_emb - mut_emb).unsqueeze(0)  # take the difference and add an extra dimension for the sequence\n","        \n","        # Set initial hidden and cell states \n","        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device) # times 2 for bidirectional\n","        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device)\n","        \n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_dim*2)\n","        \n","        # Decode the hidden state of the last time step\n","        out = self.fc1(out[:, -1, :])\n","        out = F.relu(out)\n","        out = self.dp1(out)\n","        out = self.fc2(out)\n","\n","        return out\n","\n"],"metadata":{"id":"62hRgCNyCuGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 1280 * 2  # Multiply by 2 since we're concatenating wt_emb and mut_emb\n","hidden_dim = 512\n","output_dim = 1  # ddG value\n","num_layers = 2\n","dropout_rate = 0.5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AdvancedLSTM(input_dim, hidden_dim, output_dim, num_layers, dropout_rate)\n","model = model.to(device)  # Move model to GPU if available\n"],"metadata":{"id":"spGotF_iC3iS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Oj6ewRzRC3gF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MXZn6C01C3bD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_o615ivhCt7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class OptimizedDeepFFNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(OptimizedDeepFFNN, self).__init__()\n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        \n","        self.layer2 = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim//2),\n","            nn.BatchNorm1d(hidden_dim//2),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        \n","        self.layer3 = nn.Sequential(\n","            nn.Linear(hidden_dim//2, hidden_dim//4),\n","            nn.BatchNorm1d(hidden_dim//4),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        \n","        self.final_layer = nn.Linear(hidden_dim//4, output_dim)\n","        \n","    def forward(self, wt_emb, mut_emb):\n","        x = torch.cat((wt_emb, mut_emb), dim=1)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        out = self.final_layer(x)\n","        return out\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","input_dim = 1280 * 2  # Multiply by 2 since we're concatenating wt_emb and mut_emb\n","hidden_dim = 512\n","output_dim = 1  # ddG value\n","model = OptimizedDeepFFNN(input_dim, hidden_dim, output_dim)\n","model.apply(init_weights)\n","\n","# Training loop\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","num_epochs = 15  # Adjust as needed\n","patience = 10  # Patience for early stopping\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = ReduceLROnPlateau(optimizer, 'min')\n","\n","best_val_loss = None\n","counter = 0\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_losses = []\n","\n","    for wt_emb, mut_emb, ddg in train_dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(wt_emb, mut_emb)\n","        loss = criterion(outputs, ddg)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","    \n","    avg_train_loss = sum(train_losses) / len(train_losses)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_losses = []\n","        for wt_emb, mut_emb, ddg in val_dataloader:\n","            outputs = model(wt_emb, mut_emb)\n","            loss = criterion(outputs, ddg)\n","            val_losses.append(loss.item())\n","    \n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    scheduler.step(avg_val_loss)\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n","\n","    # Early stopping\n","    if best_val_loss is None:\n","        best_val_loss = avg_val_loss\n","    elif avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        counter = 0  # Reset counter if validation loss decreases\n","    else:\n","        counter += 1  # Increase counter if validation loss doesn't decrease\n","        print(f'EarlyStopping counter: {counter} out of {patience}')\n","        if counter >= patience:\n","            print('Early stopping triggered.')\n","            break\n","\n"],"metadata":{"id":"NFlVrUgZZ_Um","colab":{"base_uri":"https://localhost:8080/"},"outputId":"960aad34-1d4d-4c42-a377-e08ee4fea411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.9982543740216652, Val Loss: 0.8239666412327248\n","Epoch 2, Train Loss: 0.8638755711170545, Val Loss: 0.8055304239725647\n","Epoch 3, Train Loss: 0.8358856649888525, Val Loss: 0.777334598498027\n","Epoch 4, Train Loss: 0.8200618472625513, Val Loss: 0.7732673348216219\n","Epoch 5, Train Loss: 0.8101099379007922, Val Loss: 0.7596597391955252\n","Epoch 6, Train Loss: 0.8005777087951608, Val Loss: 0.7454119380606251\n","Epoch 7, Train Loss: 0.7917779478657365, Val Loss: 0.74078279242843\n","Epoch 8, Train Loss: 0.78601096122388, Val Loss: 0.7329647132090785\n","Epoch 9, Train Loss: 0.7803704040507162, Val Loss: 0.7203187709752621\n","Epoch 10, Train Loss: 0.7753325609400306, Val Loss: 0.7241938034929022\n","EarlyStopping counter: 1 out of 10\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kpRChemTK2NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r2xqF9KKK2Dm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction & submission"],"metadata":{"id":"9GDKutS_nKOT"}},{"cell_type":"code","source":["# load embedding tensors & traing csv\n","wt_test_emb = torch.load(\"test/test_wt.pt\")\n","mut_test_emb = torch.load(\"test/test_mut.pt\")\n","df_test = pd.read_csv(\"test/test.csv\")"],"metadata":{"id":"ave_DDMp8fo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating testing dataset and loading the embedding\n","test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n","# preparing a dataloader for the testing\n","test_dataloader = torch.utils.data.dataloader.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        shuffle=False,\n","        num_workers=2,\n","    )"],"metadata":{"id":"9Xmav2yhm_Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=model.to(device)\n","df_result = pd.DataFrame()\n","with torch.no_grad():\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n","    x1 = data_wt.to(device)\n","    x2 = data_mut.to(device)\n","    id = target.to(device)\n","    # make prediction\n","    y_pred = model(x1,x2)\n","    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"],"metadata":{"id":"DiylsXvjqOul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result.to_csv(\"submission_deepffnn.csv\",index=False)"],"metadata":{"id":"FPm-a2USexgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7PGmPkRmezal"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}