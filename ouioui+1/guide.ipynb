{"cells":[{"cell_type":"code","source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"],"metadata":{"id":"DEk5q0rhjBWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684023724457,"user_tz":-120,"elapsed":77022,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}},"outputId":"437639e2-4b3a-48f1-cca5-365e12a68c39"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://indaba-data/README.txt...\n","/ [0 files][    0.0 B/   33.0 B]                                                \rCopying gs://indaba-data/test/test.csv...\n","Copying gs://indaba-data/test/test_mut.pt...\n","Copying gs://indaba-data/test/test_wt.pt...\n","Copying gs://indaba-data/train/train_wt.pt...\n","Copying gs://indaba-data/train/train.csv...\n","Copying gs://indaba-data/train/train_mut.pt...\n","==> NOTE: You are downloading one or more large file(s), which would\n","run significantly faster if you enabled sliced object downloads. This\n","feature is enabled by default but requires that compiled crcmod be\n","installed (see \"gsutil help crcmod\").\n","\n"]}]},{"cell_type":"code","source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","\n","# move to data folder\n","%cd indaba-data"],"metadata":{"id":"Jvd8ERpgTvji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e99df9c-a180-4c79-a82a-5796ca41f7ba","executionInfo":{"status":"ok","timestamp":1684024081692,"user_tz":-120,"elapsed":402,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/indaba-data/indaba-data\n"]}]},{"cell_type":"code","source":["# Load Embedding tensors & Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"],"metadata":{"id":"36ZgVoj5odV4","executionInfo":{"status":"ok","timestamp":1684028885720,"user_tz":-120,"elapsed":22003,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"],"metadata":{"id":"r4yfvBYXqJgS","executionInfo":{"status":"ok","timestamp":1684028902688,"user_tz":-120,"elapsed":6367,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["len(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7uKUv7u8d4a","executionInfo":{"status":"ok","timestamp":1684029956162,"user_tz":-120,"elapsed":5,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}},"outputId":"9d16a73f-31bb-4ec3-d1cf-364185513cb9"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8495"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["len(val_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFr3XT4jAfmA","executionInfo":{"status":"ok","timestamp":1684029975252,"user_tz":-120,"elapsed":6,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}},"outputId":"d28a64b9-20a4-414e-ea81-7e2d263a4a2c"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2124"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"id":"J2_X1CXQ8dcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class LSTMWithAttention(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n","        super(LSTMWithAttention, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        \n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","\n","        # Attention mechanism\n","        self.attention = nn.Linear(hidden_dim, 1)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        # Pass the embeddings through the LSTM layer\n","        _, (hidden_state, _) = self.lstm(wt_emb)\n","\n","        if isinstance(hidden_state, tuple):\n","            # If hidden_state is a tuple (hidden_state, cell_state)\n","            hidden_state = hidden_state[0]\n","\n","        # Apply attention mechanism\n","        attn_weights = torch.softmax(self.attention(hidden_state), dim=1)\n","        context_vector = torch.bmm(attn_weights.unsqueeze(2), hidden_state.unsqueeze(1)).squeeze(1)\n","\n","        # Pass the context vector through the fully connected layer\n","        output = self.fc(context_vector)\n","        \n","        return output\n","\n"],"metadata":{"id":"HrbfsC8eqJXV","executionInfo":{"status":"ok","timestamp":1684025004030,"user_tz":-120,"elapsed":1102,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate the LSTM model with attention\n","input_dim = 1280  # Assuming the input dimensions of your embeddings are 1280\n","hidden_dim = 256\n","num_layers = 1\n","output_dim = 1\n","model = LSTMWithAttention(input_dim, hidden_dim, num_layers, output_dim).to(device)\n","# Training parameters\n","epochs = 10\n","learning_rate = 0.001\n","# Define the loss function and optimizer\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    for i, (wt, mut, ddg) in enumerate(train_dataloader):\n","        wt, mut, ddg = wt.to(device), mut.to(device), ddg.to(device)\n","\n","        # Forward pass\n","        outputs = model(wt, mut)\n","        loss = criterion(outputs, ddg)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for i, (wt_val, mut_val, ddg_val) in enumerate(val_dataloader):\n","            wt_val, mut_val, ddg_val = wt_val.to(device), mut_val.to(device), ddg_val.to(device)\n","\n","            # Forward pass and calculate loss\n","            outputs = model(wt_val, mut_val)\n","            val_loss = criterion(outputs, ddg_val)\n","            val_losses.append(val_loss.item())\n","\n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item()}, Validation Loss: {avg_val_loss}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'lstm_with_attention.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"SR4uP1ikqJMz","executionInfo":{"status":"error","timestamp":1684028854301,"user_tz":-120,"elapsed":3847854,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}},"outputId":"ef5e97e8-2bfc-4b68-a555-14e9334b4032"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([14, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 1.4516680240631104, Validation Loss: 1.0927000908696718\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([20, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Train Loss: 0.7721603512763977, Validation Loss: 1.0848169151812848\n","Epoch 3/10, Train Loss: 2.2502424716949463, Validation Loss: 1.0843591267302661\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-39568fa7841d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"N-NekoVj5T_D","executionInfo":{"status":"aborted","timestamp":1684023641262,"user_tz":-120,"elapsed":42,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RBKyOZsG5TdK","executionInfo":{"status":"aborted","timestamp":1684023641262,"user_tz":-120,"elapsed":42,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"edYi37670Y07","executionInfo":{"status":"aborted","timestamp":1684023641262,"user_tz":-120,"elapsed":41,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nNEzUeNUzlKE","executionInfo":{"status":"aborted","timestamp":1684023641263,"user_tz":-120,"elapsed":42,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of training script\n","device = torch.device(\"cuda\")\n","model =  StabilityModel().to(device)\n","optimizer = torch.optim.Adadelta(model.parameters(), lr=0.0001)\n","criterion = torch.nn.MSELoss()\n","epoch_loss = 0\n","for i in range(1):\n","  epoch_loss = 0\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(train_dataloader)):\n","      # extract input from datallader\n","      x1 = data_wt.to(device)\n","      x2 = data_mut.to(device)\n","      y = target.to(device)\n","      # make prediction\n","      y_pred = model(x1,x2)\n","      # calculate loss and run optimizer\n","      loss = torch.sqrt(criterion(y, y_pred))\n","      loss.backward()\n","      optimizer.step()\n","      epoch_loss += loss\n","  print(\"epoch_\",i,\" = \", epoch_loss/len(train_dataloader))\n","  # [Recommended] Save trained models to select best checkpoint for prediction (or add prediction in the epochs loop)"],"metadata":{"id":"ndGfhMrjlmUE","executionInfo":{"status":"aborted","timestamp":1684023641264,"user_tz":-120,"elapsed":32843,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction & submission"],"metadata":{"id":"9GDKutS_nKOT"}},{"cell_type":"code","source":["# load embedding tensors & traing csv\n","wt_test_emb = torch.load(\"test/test_wt.pt\")\n","mut_test_emb = torch.load(\"test/test_mut.pt\")\n","df_test = pd.read_csv(\"test/test.csv\")"],"metadata":{"id":"ave_DDMp8fo9","executionInfo":{"status":"aborted","timestamp":1684023641264,"user_tz":-120,"elapsed":32842,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating testing dataset and loading the embedding\n","test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n","# preparing a dataloader for the testing\n","test_dataloader = torch.utils.data.dataloader.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        shuffle=False,\n","        num_workers=2,\n","    )"],"metadata":{"id":"9Xmav2yhm_Di","executionInfo":{"status":"aborted","timestamp":1684023641265,"user_tz":-120,"elapsed":32843,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result = pd.DataFrame()\n","with torch.no_grad():\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n","    x1 = data_wt.to(device)\n","    x2 = data_mut.to(device)\n","    id = target.to(device)\n","    # make prediction\n","    y_pred = model(x1,x2)\n","    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"],"metadata":{"id":"DiylsXvjqOul","executionInfo":{"status":"aborted","timestamp":1684023641265,"user_tz":-120,"elapsed":32842,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_result.to_csv(\"submission.csv\",index=False)"],"metadata":{"id":"FPm-a2USexgw","executionInfo":{"status":"aborted","timestamp":1684023641266,"user_tz":-120,"elapsed":32842,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7PGmPkRmezal","executionInfo":{"status":"aborted","timestamp":1684023641266,"user_tz":-120,"elapsed":32841,"user":{"displayName":"Hamza Labidi","userId":"13160711510787677017"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}