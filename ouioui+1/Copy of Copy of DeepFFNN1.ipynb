{"cells":[{"cell_type":"code","source":["#@title Download data from GCP bucket\n","import sys\n","\n","if 'google.colab' in sys.modules:\n","  !gsutil -m cp -r gs://indaba-data .\n","else:\n","  !mkdir -p indaba-data/train\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train.csv --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_mut.pt --continue\n","  !wget -P indaba-data/train https://storage.googleapis.com/indaba-data/train/train_wt.pt --continue\n","\n","  !mkdir -p indaba-data/test\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test.csv --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_mut.pt --continue\n","  !wget -P indaba-data/test https://storage.googleapis.com/indaba-data/test/test_wt.pt --continue"],"metadata":{"id":"DEk5q0rhjBWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Imports and moving to working directory\n","import torch \n","import pandas as pd\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","\n","\n","# move to data folder\n","%cd indaba-data"],"metadata":{"id":"Jvd8ERpgTvji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Embedding tensors & Traing csv\n","# Embeddings were calculated using the ESM 650M pretrained model \n","# Tensor shape of embedded data:  [data_len,1280] \n","# There are no sequences in the Embedding tensors as we've performed an average of it (torch.mean(embed, dim=1))\n","# More details in https://huggingface.co/facebook/esm2_t33_650M_UR50D\n","\n","wt_emb = torch.load(\"train/train_wt.pt\")\n","mut_emb = torch.load(\"train/train_mut.pt\")\n","df = pd.read_csv(\"train/train.csv\")"],"metadata":{"id":"36ZgVoj5odV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df)"],"metadata":{"id":"7O4dRi72DA_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Reset the index of the dataframe\n","df.reset_index(drop=True, inplace=True)\n","\n","# Split data into train and validation\n","wt_emb_train, wt_emb_val, mut_emb_train, mut_emb_val, df_train, df_val = train_test_split(wt_emb, mut_emb, df, test_size=0.21, random_state=42)\n","\n","# Define the dataset class\n","class EmbeddingDataset(Dataset):\n","  def __init__(self, wt_pt, mut_pt, data_df):\n","    self.pt_wt = wt_pt\n","    self.pt_mut = mut_pt\n","    self.df = data_df\n","\n","  def __len__(self):\n","    return len(self.pt_wt)\n","\n","  def __getitem__(self, index):\n","    if \"ddg\" in self.df.columns:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ddg\"]])\n","    else:\n","      df_out = torch.Tensor([self.df.iloc[index][\"ID\"]])\n","\n","    return self.pt_wt[index,:], self.pt_mut[index,:], df_out\n","\n","# Create separate datasets for the training and validation sets\n","train_dataset = EmbeddingDataset(wt_emb_train, mut_emb_train, df_train.reset_index(drop=True))\n","val_dataset = EmbeddingDataset(wt_emb_val, mut_emb_val, df_val.reset_index(drop=True))\n","\n","# Create dataloaders for the training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=27, shuffle=False, num_workers=2)\n","val_dataloader = DataLoader(val_dataset, batch_size=27, shuffle=False, num_workers=2)\n"],"metadata":{"id":"0QhdR9Tq3Acm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataloader)"],"metadata":{"id":"NOtYzkrWHPz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(val_dataloader)\n"],"metadata":{"id":"lNRejSIXHTOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (wt_emb, mut_emb, ddg) in enumerate(train_dataloader):\n","    print(f'wt_emb shape: {wt_emb.shape}, mut_emb shape: {mut_emb.shape}')\n","    if i > 5:  # Print shapes for first 5 batches only\n","        break\n"],"metadata":{"id":"qBorXcDfU8MN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for wt_emb, mut_emb, ddg in train_dataloader:\n","    print(wt_emb_train.shape, mut_emb_train.shape, ddg.shape)\n","    break\n","import torch.nn as nn\n","import torch.nn.functional as F\n"],"metadata":{"id":"sT81Nq68VqiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DeepFFNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(DeepFFNN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.dp1 = nn.Dropout(0.5)\n","        \n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.bn2 = nn.BatchNorm1d(hidden_dim)\n","        self.dp2 = nn.Dropout(0.5)\n","        \n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.bn3 = nn.BatchNorm1d(hidden_dim)\n","        self.dp3 = nn.Dropout(0.5)\n","\n","        self.fc4 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, wt_emb, mut_emb):\n","        x = torch.cat((wt_emb, mut_emb), dim=1)\n","        \n","        x = F.relu(self.bn1(self.fc1(x)))\n","        x = self.dp1(x)\n","        \n","        x = F.relu(self.bn2(self.fc2(x)))\n","        x = self.dp2(x)\n","        \n","        x = F.relu(self.bn3(self.fc3(x)))\n","        x = self.dp3(x)\n","        \n","        out = self.fc4(x)\n","        \n","        return out\n","\n","input_dim = 1280 * 2  # Multiply by 2 since we're concatenating wt_emb and mut_emb\n","hidden_dim = 512\n","output_dim = 1  # ddG value\n","\n","model = DeepFFNN(input_dim, hidden_dim, output_dim)\n"],"metadata":{"id":"NFlVrUgZZ_Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","\n","num_epochs = 20  # Adjust as needed\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_losses = []\n","\n","    for wt_emb, mut_emb, ddg in train_dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(wt_emb, mut_emb)\n","        loss = criterion(outputs, ddg)\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","    \n","    avg_train_loss = sum(train_losses) / len(train_losses)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        val_losses = []\n","        for wt_emb, mut_emb, ddg in val_dataloader:\n","            outputs = model(wt_emb, mut_emb)\n","            loss = criterion(outputs, ddg)\n","            val_losses.append(loss.item())\n","    \n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNdh0mHLT76j","outputId":"b8d13341-1afc-488f-c186-277ee2ddc1d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.9723331682508415, Val Loss: 0.829824508617784\n","Epoch 2, Train Loss: 0.8587230365266985, Val Loss: 0.7939834720678867\n","Epoch 3, Train Loss: 0.8310866027971529, Val Loss: 0.7711610849545032\n","Epoch 4, Train Loss: 0.8155968670057497, Val Loss: 0.762554890339094\n","Epoch 5, Train Loss: 0.8042504556913224, Val Loss: 0.7538969334309362\n","Epoch 6, Train Loss: 0.7948015376062154, Val Loss: 0.7397816039105956\n","Epoch 7, Train Loss: 0.7860062614463987, Val Loss: 0.7332158229295115\n","Epoch 8, Train Loss: 0.7792703882219494, Val Loss: 0.727293760373743\n","Epoch 9, Train Loss: 0.7724689734030648, Val Loss: 0.7206938563142362\n","Epoch 10, Train Loss: 0.7687712052165923, Val Loss: 0.7144088046459641\n","Epoch 11, Train Loss: 0.7632086129750052, Val Loss: 0.7094716422550164\n","Epoch 12, Train Loss: 0.759529313374947, Val Loss: 0.7062453040955331\n","Epoch 13, Train Loss: 0.7539119569768706, Val Loss: 0.7030017070198799\n","Epoch 14, Train Loss: 0.7501003997540167, Val Loss: 0.6947250690056075\n","Epoch 15, Train Loss: 0.746046076180817, Val Loss: 0.6921379174240782\n","Epoch 16, Train Loss: 0.7418414294656746, Val Loss: 0.6872814619604314\n","Epoch 17, Train Loss: 0.739485401993689, Val Loss: 0.6863568446997481\n","Epoch 18, Train Loss: 0.7351969472121195, Val Loss: 0.6815989355877679\n","Epoch 19, Train Loss: 0.7333179629261501, Val Loss: 0.6783938505829531\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1Qy8OdmpKQOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kpRChemTK2NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r2xqF9KKK2Dm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction & submission"],"metadata":{"id":"9GDKutS_nKOT"}},{"cell_type":"code","source":["# load embedding tensors & traing csv\n","wt_test_emb = torch.load(\"test/test_wt.pt\")\n","mut_test_emb = torch.load(\"test/test_mut.pt\")\n","df_test = pd.read_csv(\"test/test.csv\")"],"metadata":{"id":"ave_DDMp8fo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating testing dataset and loading the embedding\n","test_dataset = EmbeddingDataset(wt_test_emb,mut_test_emb,df_test)\n","# preparing a dataloader for the testing\n","test_dataloader = torch.utils.data.dataloader.DataLoader(\n","        test_dataset,\n","        batch_size=32,\n","        shuffle=False,\n","        num_workers=2,\n","    )"],"metadata":{"id":"9Xmav2yhm_Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model=model.to(device)\n","df_result = pd.DataFrame()\n","with torch.no_grad():\n","  for batch_idx, (data_mut,data_wt , target) in tqdm(enumerate(test_dataloader)):\n","    x1 = data_wt.to(device)\n","    x2 = data_mut.to(device)\n","    id = target.to(device)\n","    # make prediction\n","    y_pred = model(x1,x2)\n","    df_result = pd.concat([df_result, pd.DataFrame({\"ID\":id.squeeze().cpu().numpy().astype(int) , \"ddg\" : y_pred.squeeze().cpu().numpy()})])"],"metadata":{"id":"DiylsXvjqOul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result.to_csv(\"submission_deepffnn_BS26.csv\",index=False)"],"metadata":{"id":"FPm-a2USexgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7PGmPkRmezal"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}